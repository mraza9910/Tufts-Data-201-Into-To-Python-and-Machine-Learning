{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "### Name: Maida Raza\n",
    "### Collaborator:\n",
    "\n",
    "\n",
    "DATA 201\n",
    "\n",
    "Summer 2025\n",
    "\n",
    "Tufts University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework explores KNN, Decision Trees, and Random Forests. The first question reviews training a KNN model. Subsequent questions provide an in-depth examination of Gini impurity and the mechanics of training a Decision Tree. Following this, we delve into basic implementations of both Decision Trees and Random Forests, accompanied by an introduction to tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "\n",
    "(a) Load the Heart Disease dataset and name it 'df'. Conduct data cleaning.\n",
    "\n",
    "- Perform any data cleaning or data transformation steps if required\n",
    "- Explain some of the data cleaning steps which you can perform on **any** data set\n",
    "\n",
    "For clarification, please find the metadata below:\n",
    "- BPMeds: whether or not the patient was on blood pressure medication\n",
    "- prevalentStroke: whether or not the patient had previously had a stroke\n",
    "- prevalentHyp: whether or not the patient was hypertensive\n",
    "- diabetes: whether or not the patient had diabetes\n",
    "- totChol: total cholesterol level\n",
    "- sysBP: systolic blood pressure\n",
    "- diaBP: diastolic blood pressure\n",
    "- BMI: Body Mass Index\n",
    "- heartRate: heart rate\n",
    "- glucose: glucose level\n",
    "- 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”) (Predictor Variable)\n",
    "\n",
    "(b) Create two dataframes for features and target variable (TenYearCHD). Conduct a full model training and testing (30%) for a KNN model with K=5. Print the accuracy, confusion matrix, and explain what the confuison matrix tells you\n",
    "\n",
    "(c) Create a range from 1 to 50 going at steps of 2 then make a list where you will store average accuracy at each k value\n",
    "use a for loop to compute the average accuracy over 10-fold cross validation for each k value. Plot the average accuracy for each k values and determine which k value you will choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(a)\n",
    "\n",
    "* Perform the data cleaning steps, as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>25.97</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19.71</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.16</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.47</td>\n",
       "      <td>80.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4238 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "0        1   39        4.0              0         0.0     0.0   \n",
       "1        0   46        2.0              0         0.0     0.0   \n",
       "2        1   48        1.0              1        20.0     0.0   \n",
       "3        0   61        3.0              1        30.0     0.0   \n",
       "4        0   46        3.0              1        23.0     0.0   \n",
       "...    ...  ...        ...            ...         ...     ...   \n",
       "4233     1   50        1.0              1         1.0     0.0   \n",
       "4234     1   51        3.0              1        43.0     0.0   \n",
       "4235     0   48        2.0              1        20.0     NaN   \n",
       "4236     0   44        1.0              1        15.0     0.0   \n",
       "4237     0   52        2.0              0         0.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "0                   0             0         0    195.0  106.0   70.0  26.97   \n",
       "1                   0             0         0    250.0  121.0   81.0  28.73   \n",
       "2                   0             0         0    245.0  127.5   80.0  25.34   \n",
       "3                   0             1         0    225.0  150.0   95.0  28.58   \n",
       "4                   0             0         0    285.0  130.0   84.0  23.10   \n",
       "...               ...           ...       ...      ...    ...    ...    ...   \n",
       "4233                0             1         0    313.0  179.0   92.0  25.97   \n",
       "4234                0             0         0    207.0  126.5   80.0  19.71   \n",
       "4235                0             0         0    248.0  131.0   72.0  22.00   \n",
       "4236                0             0         0    210.0  126.5   87.0  19.16   \n",
       "4237                0             0         0    269.0  133.5   83.0  21.47   \n",
       "\n",
       "      heartRate  glucose  TenYearCHD  \n",
       "0          80.0     77.0           0  \n",
       "1          95.0     76.0           0  \n",
       "2          75.0     70.0           0  \n",
       "3          65.0    103.0           1  \n",
       "4          85.0     85.0           0  \n",
       "...         ...      ...         ...  \n",
       "4233       66.0     86.0           1  \n",
       "4234       65.0     68.0           0  \n",
       "4235       84.0     86.0           0  \n",
       "4236       86.0      NaN           0  \n",
       "4237       80.0    107.0           0  \n",
       "\n",
       "[4238 rows x 16 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1. Load the Heart Disease dataset and name it 'df'. \n",
    "df = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Heart Disease Data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>105</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>currentSmoker</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cigsPerDay</th>\n",
       "      <td>29</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BPMeds</th>\n",
       "      <td>53</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentStroke</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevalentHyp</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diabetes</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totChol</th>\n",
       "      <td>50</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sysBP</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diaBP</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BMI</th>\n",
       "      <td>19</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heartRate</th>\n",
       "      <td>1</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glucose</th>\n",
       "      <td>388</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TenYearCHD</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Missing Values Data Type  Duplicates\n",
       "male                          0     int64           0\n",
       "age                           0     int64           0\n",
       "education                   105   float64           0\n",
       "currentSmoker                 0     int64           0\n",
       "cigsPerDay                   29   float64           0\n",
       "BPMeds                       53   float64           0\n",
       "prevalentStroke               0     int64           0\n",
       "prevalentHyp                  0     int64           0\n",
       "diabetes                      0     int64           0\n",
       "totChol                      50   float64           0\n",
       "sysBP                         0   float64           0\n",
       "diaBP                         0   float64           0\n",
       "BMI                          19   float64           0\n",
       "heartRate                     1   float64           0\n",
       "glucose                     388   float64           0\n",
       "TenYearCHD                    0     int64           0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Conduct data cleaning and explain \n",
    "\n",
    "# Check for missing observations\n",
    "missing_observations = df.isnull().sum()\n",
    "\n",
    "# Check the data type for each column\n",
    "data_type = df.dtypes\n",
    "\n",
    "# Duplicated rows\n",
    "duplicates = df.duplicated().sum()\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Missing Values': missing_observations,\n",
    "    'Data Type': data_type,\n",
    "    'Duplicates': duplicates\n",
    "})\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Comments and Next Steps:\n",
    "\n",
    "* I performed the data cleaning steps and here are the key insights:\n",
    "\n",
    "    1. Missing Data: Checking for missing data and deciding how to handle it (replace with mean/median). \n",
    "        - Missing observations for: cigsPerDay, BPMeds, TotalCol, BMI, Heartrate, Glucose. Will replace the missing observations with the column median. I will drop the column Glucose as that has many missing observations\n",
    "\n",
    "    2. Check for duplicates: Check for and remove any duplicates\n",
    "        - No duplicates were found in any features\n",
    "\n",
    "    3. Data Type: Ensure that columns with string/integer variables have consistent data type\n",
    "        - Data type seems consistent for every feature\n",
    "\n",
    "    4. Outlier Detection: Identify and address outliers\n",
    "\n",
    "    5. Dropping Irrelevant Features: Remove the columns not useful for our analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "education          0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "TenYearCHD         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling in missing observations with their column median\n",
    "df.fillna(df.median(numeric_only=True), inplace=True) # putting inplace because we want to replace values without creating new copies\n",
    "df.drop('glucose', axis='columns') # need to specify axis = 'rows/column\n",
    "\n",
    "# Verify that all columns have been cleaned\n",
    "cleaned_df = df.isnull().sum()\n",
    "cleaned_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The data cleaning process is complete. All missing values have been filled using the median, and there are no remaining missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(b):\n",
    "\n",
    "* Create two dataframes for features and target variable (TenYearCHD). Conduct a full model training and testing (30%) for a KNN model with K=5. Print the accuracy, confusion matrix, and explain what the confusion matrix tells you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix is [[1045   39]\n",
      " [ 168   20]]\n",
      "Accuracy Score is 0.8372641509433962\n"
     ]
    }
   ],
   "source": [
    "# Q1. b. Create two dataframes for features and target variable (TenYearCHD). In here, I will assume that all other columns are the predictor variables\n",
    "# Conduct a full model training and testing (30%) for a KNN model with K=5. Print the accuracy, confusion matrix, and explain what the confusion matrix tells you\n",
    "\n",
    "y = df['TenYearCHD']\n",
    "x = df.drop('TenYearCHD', axis= 'columns')\n",
    "\n",
    "# Standardize the feature data - transforming the data to have zero mean and unit variance. This helps improve performance for algorithms like KNN, Vector Machines, and Gradient Boost.\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(x)\n",
    "\n",
    "# Dividing into test and train data\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Now running the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(x_train, y_train)\n",
    "y_predict = knn_model.predict(x_test)\n",
    "\n",
    "# Calculating confusion matrix and the accuracy score\n",
    "con_mat = confusion_matrix(y_test, y_predict)\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(f'Confusion Matrix is {con_mat}')\n",
    "print(f'Accuracy Score is {acc_score}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Analysis:\n",
    "\n",
    "1. Accuracy Score:\n",
    "    - The model has an accuracy score of 0.8372641509433962, meaning that the KNN model achieved an accuracy of ~84% on the test dataset.\n",
    "\n",
    "2. Confusion Matrix: I will assume that negative means the patient not getting CHD (0) and positive means the patient getting CHD (1)\n",
    "\n",
    "    - Here we were using the KNN model to predict the risk of coronary heart disease, given a sample of data. Our model learned from the x_train dataset to create predictions shown in the Confusion Matrix above.\n",
    "    - True Negative: KNN model accurately predicts 1045 instances of patient not getting CHD (0). \n",
    "    - False Positive: KNN model inaccurately predicted 39 instances of patient getting CHD (1) when the actual outcome is 0.\n",
    "    - False Negative: KNN model inaccurately predicted that 168 patients wont get CHD when the actual outcome is 1, meaning that they will get CHD\n",
    "    - True Positive: KNN model accurately predicted 20 patient getting CHD, who will actually get it.\n",
    "\n",
    "The model performs well in determining individuals without risk, but it struggles with identifying individuals at risk of getting CHD, reflected in a high False Negative number. This suggests potential class imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1(c):\n",
    "\n",
    "* Create a range from 1 to 50 going at steps of 2 then make a list where you will store average accuracy at each k value\n",
    "use a for loop to compute the average accuracy over 10-fold cross validation for each k value. Plot the average accuracy for each k values and determine which k value you will choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K-Value at 5-Fold is 11 \n",
      "Best K-Value at 10-Fold is 13 \n"
     ]
    }
   ],
   "source": [
    "# Create a range from 1 to 50 going at steps of 2 then make a list where you will store average accuracy at each k value\n",
    "# use a for loop to compute the average accuracy over 10-fold cross validation for each k value. Plot the average accuracy for each k values and determine which k value you will choose.\n",
    "# I am also curious to know if 10 cross validation is better than 5 fold cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score # evaluates model performance through cross validation, divides the data into test and split and calculates the score. \n",
    "k_values = range(1,51,2)\n",
    "average_accuracy_10 = []\n",
    "average_accuracy_5 = []\n",
    "k_vals = []\n",
    "\n",
    "for k in k_values:\n",
    "    k_vals.append(k)\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    score_10 = cross_val_score(model, scaled_data, y, cv=10)\n",
    "    score_5 = cross_val_score(model, scaled_data, y, cv=5)\n",
    "    average_accuracy_10.append(score_10.mean())\n",
    "    average_accuracy_5.append(score_5.mean())\n",
    "    \n",
    "# Creating the plot now - essentially we are trying to see which k-values give us the best score\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(k_vals, average_accuracy_10, marker = 'o', color = 'orange', label = '10 Cross Fold Validation')\n",
    "plt.plot(k_vals, average_accuracy_5, marker = 'x', color = 'black', label = '5 Cross Fold Validation')\n",
    "plt.title('Average Model Accuracy for Different K Values (KNN)')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Average Model Accuracy')\n",
    "plt.xticks(k_vals)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# best KValues\n",
    "bestK_10fold = k_values[average_accuracy_10.index(max(average_accuracy_10))]\n",
    "bestK_5fold = k_values[average_accuracy_5.index(max(average_accuracy_5))]\n",
    "print(f'Best K-Value at 5-Fold is {bestK_5fold} ')\n",
    "print(f'Best K-Value at 10-Fold is {bestK_10fold} ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* The goal of Q2 is to make a function that can calculate the weighted gini impurity over any grouping and any size of classes.\n",
    "\n",
    "    (a) Calculate the gini impurity of the example by hand. Then write some code to do it for you. You will have to find $p_0$ and $p_1$ which are the probabilities of selecting a 0 and a 1 (respectively) from the group.\n",
    "\n",
    "    (b) Now say we have two groups? Calculate the gini index of each group. Now, make a weighted sum of the gini numbers, each weighted by the proportion of the group size to the total number of entries (i.e. if group 1 is of length 10 and group 2 is of length 15, then group 1 would have a weight of 2/5 and group 2 a weight of 3/5).\n",
    "\n",
    "    (c) Generalize that bit of code you wrote to now deal with any number of groups with any number of classes. Assume the groups will be given as a list of lists, and if there is an empty group the gini number will be 0 (there needs to be an if statement to make sure we do not divide by 0).\n",
    "\n",
    "    (d) Make that bit of code into a function called gini_imp which takes a 'groups' variable and a 'classes' variable. It should return the gini index. Note, you need to use the classes variable because that is how you know what classes to check for in each group. Test it on the same example from part (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(a)\n",
    "\n",
    "* Calculate the gini impurity of the example by hand. Then write some code to do it for you. You will have to find $p_0$ and $p_1$ which are the probabilities of selecting a 0 and a 1 (respectively) from the group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 (a)\n",
    "\n",
    "###This is the example\n",
    "classes = [0,1]\n",
    "group = [0,0,0,1,1,0,1,0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444444444444444"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2(a) Gini Impurity tells us how often a randomly chosen data point would be misclassified\n",
    "# Code for calculating Gini Impurity for the data above:\n",
    "# This function takes the total length of a group, accesses the data observations inside it and then assign probabilities\n",
    "\n",
    "def gini_impurity(group):\n",
    "\n",
    "    sample_size = len(group)\n",
    "    zeros = group.count(0)\n",
    "    ones = group.count(1)\n",
    "\n",
    "    p0 = (zeros / sample_size) ** 2\n",
    "    p1 = (ones / sample_size) ** 2\n",
    "    \n",
    "    Gini_Impurity = 1 - (p0 + p1)\n",
    "    return Gini_Impurity\n",
    "\n",
    "group = [0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
    "gini_impurity(group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(a) - By Hand:\n",
    "\n",
    "The Gini Impurity measures the impurity or heterogeneity of a group. It is calculated using the formula:\n",
    "\n",
    "Gini Impurity = 1 - (p0^2 + p1^2)\n",
    "\n",
    "Where:\n",
    "* p0 is the probability of selecting a 0 from the group.\n",
    "* p1 is the probability of selecting a 1 from the group.\n",
    "\n",
    "Given the group: [0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
    "* Total number of elements: n = 9\n",
    "* Number of 0's: n0 = 6\n",
    "* Number of 1's: n1 = 3\n",
    "\n",
    "Now we can find the probabilities:\n",
    "* p0 = n0/n = 6/9 = 0.6667\n",
    "* p1 = n1/n = 3/9 = 0.3333\n",
    "\n",
    "Using the formula Gini Impurity = 1 - (p0^2 + p1^2):\n",
    "* Gini Impurity = 1 - (0.6667^2 + 0.3333^2)\n",
    "* Gini Impurity = 1 - (0.4444 + 0.1111) = 1 - 0.5555 = 0.4444\n",
    "\n",
    "Thus, the Gini Impurity of the group = [0, 0, 0, 1, 1, 0, 1, 0, 0] is 0.4444.\n",
    "\n",
    "* Gini impurity = 0 (perfectly pure)\n",
    "* Gini Impurity = 0.5 (Maximum Impurity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(b)\n",
    "\n",
    "* Now say we have two groups? Calculate the gini index of each group. Now, make a weighted sum of the gini numbers, each weighted by the proportion of the group size to the total number of entries (i.e. if group 1 is of length 10 and group 2 is of length 15, then group 1 would have a weight of 2/5 and group 2 a weight of 3/5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Gini Index is 0.4375\n"
     ]
    }
   ],
   "source": [
    "# 2(b) Now say we have two groups. Calculate the gini index of each group. Now, make a weighted sum of the gini numbers, each weighted by the proportion of the group size to the total number of entries \n",
    "# # (i.e. if group 1 is of length 10 and group 2 is of length 15, then group 1 would have a weight of 2/5 and group 2 a weight of 3/5).\n",
    "\n",
    "classes = [0,1]\n",
    "groups =([0, 0, 0, 1], [0, 0, 1, 1])\n",
    "\n",
    "def gini_impurity(groups):\n",
    "    total_length = 0\n",
    "    total_weighted_gini = []\n",
    "    gini_index = 0\n",
    "    gini_sum = 0\n",
    "    \n",
    "    for m in groups:\n",
    "        total_length += len(m)\n",
    "    \n",
    "    for i in groups:\n",
    "        group_len = len(i)\n",
    "        zeros = i.count(0)\n",
    "        ones = i.count(1)\n",
    "        p_0 = (zeros / group_len ) **2\n",
    "        p_1 = (ones/group_len) ** 2\n",
    "        gini_index = (1 - (p_0 + p_1))\n",
    "        weight = group_len / total_length\n",
    "        weighted_gini_index = (weight) * gini_index\n",
    "        total_weighted_gini.append(weighted_gini_index)\n",
    "        \n",
    "    for p in total_weighted_gini:\n",
    "        gini_sum += p\n",
    "    print(f'Weighted Gini Index is {gini_sum}')\n",
    "\n",
    "gini_impurity(groups)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(c)\n",
    "\n",
    "* Generalize that bit of code you wrote to now deal with any number of groups with any number of classes. Assume the groups will be given as a list of lists, and if there is an empty group the gini number will be 0 (there needs to be an if statement to make sure we do not divide by 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Gini Index is 0.5684\n"
     ]
    }
   ],
   "source": [
    "# 2(c) Generalize that bit of code you wrote to now deal with any number of groups with any number of classes. \n",
    "# Assume the groups will be given as a list of lists, and if there is an empty group the gini number will be 0 (there needs to be an if statement to make sure we do not divide by 0).\n",
    "\n",
    "classes = [0,1,2,3]\n",
    "groups =[[0,3,1,1,1],[0,0,0,1],[2,3,1,1],[],[0,0,1,1,2,2]]\n",
    "\n",
    "def gini_impurity(groups, classes):\n",
    "    \n",
    "    total_length = 0\n",
    "    gini_index = 0\n",
    "    \n",
    "    for m in groups:\n",
    "        total_length += len(m) # you are getting the total length of the set\n",
    "    print(total_length) # total length, i.e., data observations in the set. For coder checking purposes only\n",
    "    \n",
    "    for i in groups:\n",
    "        if len(i) != 0:\n",
    "            group_length = len(i)\n",
    "            probabilities = [i.count(c) for c in classes]\n",
    "            group_weight = group_length / total_length\n",
    "        \n",
    "            for p in probabilities:\n",
    "                calculation = (p/group_length) ** 2\n",
    "                gini = calculation * group_weight\n",
    "                gini_index = gini_index + gini\n",
    "        else:\n",
    "            gini_index += 0\n",
    "    print(f'Gini Index is {1-gini_index:.4f}')\n",
    "\n",
    "gini_impurity(groups, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2(d)\n",
    "\n",
    "* Make that bit of code into a function called gini_imp which takes a 'groups' variable and a 'classes' variable. It should return the gini index. Note, you need to use the classes variable because that is how you know what classes to check for in each group. Test it on the same example from part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "Gini Index: 0.5684\n"
     ]
    }
   ],
   "source": [
    "# 2(d) Make that bit of code into a function called gini_imp which takes a 'groups' variable and a 'classes' variable. \n",
    "# It should return the gini index. Note, you need to use the classes variable because that is how you know what classes to check for in each group. Test it on the same example from part (c)\n",
    "\n",
    "def gini_imp(groups, classes):\n",
    "    def gini_impurity(groups, classes):\n",
    "    \n",
    "        total_length = 0\n",
    "        total_weighted_gini = []\n",
    "        gini_sum = 0\n",
    "        \n",
    "        for m in groups:\n",
    "            total_length = len(m) + total_length # you are getting the total length of the set\n",
    "        print(total_length) # total length, i.e., data observations in the set. For coder checking purposes\n",
    "        \n",
    "        for i in groups:\n",
    "            if len(i) != 0:\n",
    "                group_length = len(i)\n",
    "                probabilities = [i.count(c) for c in classes]\n",
    "                group_weight = group_length / total_length\n",
    "            \n",
    "                for p in probabilities:\n",
    "                    calculation = (p/group_length) ** 2\n",
    "                    gini = calculation * group_weight\n",
    "                    gini_sum = gini_sum + gini\n",
    "            else:\n",
    "                gini_sum = 0 + gini_sum \n",
    "        return(1-gini_sum)\n",
    "    return gini_impurity(groups, classes)\n",
    "\n",
    "gini_index = gini_imp(groups, classes)\n",
    "print(f'Gini Index: {gini_index:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "(a) Load the datasets titanic_train_data and titanic_test_data into dataframes and split them into X_train, X_test, y_train and y_test (the training sets are coming from the titanic train data and visa versa). Note, the first two columns should be deleted because they are just passenger Id's, also documentation about the data can be found here:https://www.kaggle.com/azeembootwala/titanic. Create a Decision Tree instance, fit the data on the training sets and get the accuracy score on the test set. Report the confusion matrix.\n",
    "\n",
    "(b) Split the X_train and y_train data into sets called X_train,X_val, y_train, y_val using train_test_split. We are going to try and tune some parameters in the Decision tree. Call a new Decision Tree instance with the following parameters: **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). What do these parameters do? (Look them up in the documentation)\n",
    "\n",
    "(c) We are going to use the validation sets to try and find the best parameter combinations. So, use a triple for loop to iterate over different ranges for each of the three parameters, find what combination gives the best accuracy on the validation set. Then, use that combination on a decision tree to classify the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3(a)\n",
    "\n",
    "* (a) Load the datasets titanic_train_data and titanic_test_data into dataframes and split them into X_train, X_test, y_train and y_test (the training sets are coming from the titanic train data and visa versa). Note, the first two columns should be deleted because they are just passenger Id's, also documentation about the data can be found here:https://www.kaggle.com/azeembootwala/titanic. Create a Decision Tree instance, fit the data on the training sets and get the accuracy score on the test set. Report the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Family_size</th>\n",
       "      <th>Title_1</th>\n",
       "      <th>Title_2</th>\n",
       "      <th>Title_3</th>\n",
       "      <th>Title_4</th>\n",
       "      <th>Emb_1</th>\n",
       "      <th>Emb_2</th>\n",
       "      <th>Emb_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>787</td>\n",
       "      <td>788</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>788</td>\n",
       "      <td>789</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.040160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>789</td>\n",
       "      <td>790</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.154588</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>790</td>\n",
       "      <td>791</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3500</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>791</td>\n",
       "      <td>792</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.050749</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  PassengerId  Survived  Sex     Age      Fare  Pclass_1  \\\n",
       "0             0            1         0    1  0.2750  0.014151         0   \n",
       "1             1            2         1    0  0.4750  0.139136         1   \n",
       "2             2            3         1    0  0.3250  0.015469         0   \n",
       "3             3            4         1    0  0.4375  0.103644         1   \n",
       "4             4            5         0    1  0.4375  0.015713         0   \n",
       "..          ...          ...       ...  ...     ...       ...       ...   \n",
       "787         787          788         0    1  0.1000  0.056848         0   \n",
       "788         788          789         1    1  0.0125  0.040160         0   \n",
       "789         789          790         0    1  0.5750  0.154588         1   \n",
       "790         790          791         0    1  0.3500  0.015127         0   \n",
       "791         791          792         0    1  0.2000  0.050749         0   \n",
       "\n",
       "     Pclass_2  Pclass_3  Family_size  Title_1  Title_2  Title_3  Title_4  \\\n",
       "0           0         1          0.1        1        0        0        0   \n",
       "1           0         0          0.1        1        0        0        0   \n",
       "2           0         1          0.0        0        0        0        1   \n",
       "3           0         0          0.1        1        0        0        0   \n",
       "4           0         1          0.0        1        0        0        0   \n",
       "..        ...       ...          ...      ...      ...      ...      ...   \n",
       "787         0         1          0.5        0        0        1        0   \n",
       "788         0         1          0.3        0        0        1        0   \n",
       "789         0         0          0.0        1        0        0        0   \n",
       "790         0         1          0.0        1        0        0        0   \n",
       "791         1         0          0.0        1        0        0        0   \n",
       "\n",
       "     Emb_1  Emb_2  Emb_3  \n",
       "0        0      0      1  \n",
       "1        1      0      0  \n",
       "2        0      0      1  \n",
       "3        0      0      1  \n",
       "4        0      0      1  \n",
       "..     ...    ...    ...  \n",
       "787      0      1      0  \n",
       "788      0      0      1  \n",
       "789      1      0      0  \n",
       "790      0      1      0  \n",
       "791      0      0      1  \n",
       "\n",
       "[792 rows x 17 columns]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Decision Tree models can be used to analyze whether a customer will purchase a product based on their age, income, and purchase history.'\n",
    "'In this one, you may start with income > 100k if advertising an expensive product. Decision tree can also be used for credit scoring and fraud detection'\n",
    "'Tree Depth is the number of steps the algorithm takes to make a decision'\n",
    "\n",
    "#Load the necessary libraries:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the datasets\n",
    "train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for Decision Tree Model is 0.8\n",
      "The Confusion Matrix for Decision Tree Model is \n",
      "[[52 12]\n",
      " [ 8 28]]\n"
     ]
    }
   ],
   "source": [
    "#Checking if observations are missing\n",
    "\n",
    "train_missing_observations = train.isnull().sum()\n",
    "test_missing_observations = train.isnull().sum()\n",
    "\n",
    "train_duplicates = train.duplicated()\n",
    "test_duplicates = test.duplicated()\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'missing_observations': train_missing_observations,\n",
    "    'test missing obs': test_missing_observations,\n",
    "    'train duplicates': train_duplicates,\n",
    "    'test duplicates': test_duplicates\n",
    "})\n",
    "\n",
    "\n",
    "# Dropping the first two columns\n",
    "train = train.iloc[:, 2:]\n",
    "test = test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the features\n",
    "x_train = train.drop('Survived', axis=1)\n",
    "y_train = train['Survived']\n",
    "\n",
    "x_test = test.drop('Survived', axis =1)\n",
    "y_test = test['Survived']\n",
    "\n",
    "# Building the Decision Tree Model\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(x_train, y_train)\n",
    "y_test_predict = tree.predict(x_test)\n",
    "\n",
    "# Getting Accuracy Score and Confusion Matrix on the Test Dataset\n",
    "accuracy_score = accuracy_score(y_test, y_test_predict)\n",
    "confusion_matrix = confusion_matrix(y_test, y_test_predict)\n",
    "\n",
    "# Printing the output\n",
    "print(f'The Accuracy Score for Decision Tree Model is {accuracy_score}')\n",
    "print(f'The Confusion Matrix for Decision Tree Model is \\n{confusion_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "1. Accuracy Score: \n",
    "\n",
    "    - The accuracy of the decision tree model on the titanic dataset is 0.8, meaning that the model is making correct predictions about passenger survival 80% of the time\n",
    "\n",
    "2. Confusion Matrix\n",
    "\n",
    "    - True Negative: 52 - for 52 observations, our model correctly identified people as not surviving\n",
    "    - False Positive: 12 - for 12 observations, our model predicted people to survive, but they did not\n",
    "    - False Negative: 8 - predicted 8 passengers to not survive when they actually survived\n",
    "    - True Positive: 28 - predicted 28 passenger to survive when they actually survive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3(b)\n",
    "\n",
    "* Split the X_train and y_train data into sets called X_train,X_val, y_train, y_val using train_test_split. We are going to try and tune some parameters in the Decision tree. Call a new Decision Tree instance with the following parameters: **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). What do these parameters do? (Look them up in the documentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[224  21]\n",
      " [ 41 110]] \n",
      " Accuracy Score: 0.8434343434343434\n"
     ]
    }
   ],
   "source": [
    "# 3(b) (b) Split the X_train and y_train data into sets called X_train,X_val, y_train, y_val using train_test_split. \n",
    "# We are going to try and tune some parameters in the Decision tree. Called a new Decision Tree instance with the following parameters: \n",
    "# **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). \n",
    "# What do these parameters do? (Look them up in the documentation)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "#print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Building New Decision Tree instance\n",
    "# Max Depth: determines how many steps a decision tree takes. Note that the higher the depth, there is a chance of overfitting because the model trained too well\n",
    "# Max Features: you set a number of features that the model should consider. The model, now decides which features are important. \n",
    "# Decision trees can overemphasize high-cardinality categorical features or numerical features with many possible splits\n",
    "# Min Sample Leaf: A leaf note is the one where the tree stops and makes a prediction. Essentially, specifies the minimum number of observations that should be there before prediction\n",
    "\n",
    "New_Tree = DecisionTreeClassifier(max_features='log2', max_depth=5, min_samples_leaf=8)\n",
    "New_Tree.fit(x_train, y_train)\n",
    "y_predict = New_Tree.predict(X_val)\n",
    "\n",
    "acc_score = accuracy_score(Y_val, y_predict)\n",
    "conf_matrix = confusion_matrix(Y_val, y_predict)\n",
    "print(f'Confusion Matrix: \\n{conf_matrix} \\n Accuracy Score: {acc_score}')\n",
    "\n",
    "\n",
    "# Printing the accuracy score\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=None, max_depth=5, min_samples_leaf=1) is {acc_score}') 0.87\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=None, max_depth=5, min_samples_leaf=8) is {acc_score}') 0.85 -> Score went down because I kept everything same, but required the model to have more instances before making a prediction\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=8, max_depth=8, min_samples_leaf=1) is {acc_score}') 0.89\n",
    "#print(f'Accuracy Score under Decision Tree Model (max_features=None, max_depth=5, min_samples_leaf=8) is {acc_score}') 0.85\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "With the chosen parameters, the accuracy score on the validation set is 84.3%.\n",
    "\n",
    "Explanation of the Parameters:\n",
    "* max_features: This parameter controls the number of features considered for splitting at each node. Using SQRT means that we are limiting the model to only consider a few features. This prevents overfitting\n",
    "* max_depth: This parameter sets the maximum depth of the tree. Limiting the depth can prevent the model from becoming overly complex, reducing the risk of overfitting.\n",
    "* min_samples_leaf: This defines the minimum number of samples required to be at a leaf node. By setting a higher value, we ensure that leaf nodes have more samples, which helps in creating a more generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3(c)\n",
    "\n",
    "* We are going to use the validation sets to try and find the best parameter combinations. So, use a triple for loop to iterate over different ranges for each of the three parameters, find what combination gives the best accuracy on the validation set. Then, use that combination on a decision tree to classify the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for Decision Tree at max_depth= 3, min_samples_leaf= 6 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_depth= 3, min_samples_leaf= 7 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_depth= 3, min_samples_leaf= 8 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_depth= 3, min_samples_leaf= 9 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_depth= 3, min_samples_leaf= 10 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_depth= 4, min_samples_leaf= 6 is 0.8611111111111112\n",
      "The Accuracy Score for Decision Tree at max_depth= 4, min_samples_leaf= 7 is 0.8611111111111112\n",
      "The Accuracy Score for Decision Tree at max_depth= 4, min_samples_leaf= 8 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_depth= 4, min_samples_leaf= 9 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_depth= 4, min_samples_leaf= 10 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_depth= 5, min_samples_leaf= 6 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_depth= 5, min_samples_leaf= 7 is 0.8560606060606061\n",
      "The Accuracy Score for Decision Tree at max_depth= 5, min_samples_leaf= 8 is 0.8585858585858586\n",
      "The Accuracy Score for Decision Tree at max_depth= 5, min_samples_leaf= 9 is 0.8585858585858586\n",
      "The Accuracy Score for Decision Tree at max_depth= 5, min_samples_leaf= 10 is 0.8585858585858586\n",
      "The Accuracy Score for Decision Tree at max_depth= 6, min_samples_leaf= 6 is 0.8686868686868687\n",
      "The Accuracy Score for Decision Tree at max_depth= 6, min_samples_leaf= 7 is 0.8686868686868687\n",
      "The Accuracy Score for Decision Tree at max_depth= 6, min_samples_leaf= 8 is 0.8686868686868687\n",
      "The Accuracy Score for Decision Tree at max_depth= 6, min_samples_leaf= 9 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_depth= 6, min_samples_leaf= 10 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_depth= 7, min_samples_leaf= 6 is 0.8686868686868687\n",
      "The Accuracy Score for Decision Tree at max_depth= 7, min_samples_leaf= 7 is 0.8712121212121212\n",
      "The Accuracy Score for Decision Tree at max_depth= 7, min_samples_leaf= 8 is 0.8737373737373737\n",
      "The Accuracy Score for Decision Tree at max_depth= 7, min_samples_leaf= 9 is 0.8636363636363636\n",
      "The Accuracy Score for Decision Tree at max_depth= 7, min_samples_leaf= 10 is 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "# 3(c) We are going to use the validation sets to try and find the best parameter combinations. \n",
    "# So, use a triple for loop to iterate over different ranges for each of the three parameters, find what combination gives the best accuracy on the validation set. \n",
    "# Then, use that combination on a decision tree to classify the test set.\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "#print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Building New Decision Tree instance\n",
    "# Max Depth: determines how many steps a decision tree takes. Note that the higher the depth, there is a chance of overfitting because the model trained too well\n",
    "# Max Features: you set a number of features that the model should consider. The model, now decides which features are important. \n",
    "# Decision trees can overemphasize high-cardinality categorical features or numerical features with many possible splits\n",
    "# Min Sample Leaf: A leaf note is the one where the tree stops and makes a prediction. Essentially, specifies the minimum number of observations that should be there before prediction\n",
    "\n",
    "for m in range(3,8):\n",
    "    for j in range(6,11):\n",
    "        New_Tree = DecisionTreeClassifier(max_depth=m, min_samples_leaf=j)\n",
    "        New_Tree.fit(x_train, y_train)\n",
    "        y_predict = New_Tree.predict(X_val)\n",
    "        acc_score = accuracy_score(Y_val, y_predict)\n",
    "        print(f'The Accuracy Score for Decision Tree at max_depth= {m}, min_samples_leaf= {j} is {acc_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters Found: {'min_samples_leaf': 10, 'max_features': None, 'max_depth': 7}\n"
     ]
    }
   ],
   "source": [
    "# Another way of determining the best parameters\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "#print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Defining the dataframe\n",
    "param_dist = {\n",
    "    'max_depth': list(range(3,8)),\n",
    "    'max_features': ['sqrt', None, 'log2'],\n",
    "    'min_samples_leaf': list(range(6,11)) \n",
    "}\n",
    "\n",
    "# Defining the model\n",
    "model = DecisionTreeClassifier()\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = model,\n",
    "    param_distributions = param_dist,\n",
    "    cv =3,\n",
    "    n_iter = 20,\n",
    "    n_jobs = 1,\n",
    "    scoring = 'accuracy'\n",
    ")\n",
    "\n",
    "# Fitting the model\n",
    "random_search.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameter\n",
    "print('Best Parameters Found:', random_search.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "The best parameter combination found on the validation set is:\n",
    "* max_features: None\n",
    "* max_depth: 7\n",
    "* min_samples_leaf: 9\n",
    "\n",
    "This achieved a validation accuracy of 86.3%. There are a few models above that achieve an accuracy score of 87%. However, we should be vary of such high accuracy scores because that risks overfitting!\n",
    "\n",
    "Now, I will use this combination to fit a Decision Tree on the training data and evaluate its performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix [[222  23]\n",
      " [ 31 120]]\n",
      "Accuracy Score = 0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "# **max_features, max_depth, and min_samples_leaf**. Choose some value to test them with, and run the model a few times to see if you can get different accuracy scores (use the X_val and y_val to get the accuracy scores). \n",
    "# What do these parameters do? (Look them up in the documentation)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_train = train.drop('Survived', axis =1)\n",
    "y_train = train['Survived']\n",
    "#print(len(x_train), len(y_train))\n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.5, random_state=42)\n",
    "\n",
    "# Building New Decision Tree instance\n",
    "# Max Depth: determines how many steps a decision tree takes. Note that the higher the depth, there is a chance of overfitting because the model trained too well\n",
    "# Max Features: you set a number of features that the model should consider. The model, now decides which features are important. \n",
    "# Decision trees can overemphasize high-cardinality categorical features or numerical features with many possible splits\n",
    "# Min Sample Leaf: A leaf note is the one where the tree stops and makes a prediction. Essentially, specifies the minimum number of observations that should be there before prediction\n",
    "\n",
    "New_Tree = DecisionTreeClassifier(max_features=None, max_depth=7, min_samples_leaf=9)\n",
    "New_Tree.fit(x_train, y_train)\n",
    "y_predict = New_Tree.predict(X_val)\n",
    "\n",
    "acc_score = accuracy_score(Y_val, y_predict)\n",
    "conf_matrix = confusion_matrix(Y_val, y_predict)\n",
    "\n",
    "print(f'Confusion Matrix {conf_matrix}')\n",
    "print(f'Accuracy Score = {acc_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "Using the best parameter combination, the accuracy on the test set is 86.3%\n",
    "\n",
    "The confusion matrix indicates:\n",
    "* 222 true negatives compared with 52 for the prior model (correctly predicted as not survived)\n",
    "* 23 false positives compared with 12 for the prior model (incorrectly predicted as survived)\n",
    "* 31 false negatives compared with 8 in the prior model (incorrectly identified as not surviving)\n",
    "* 120 true positive compared with 28 for the prior model \n",
    "\n",
    "This model performs well in both Confusion Matrix and Accuracy score when compared with the untuned Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "Let's move onto random forests, we'll be doing more parameter tuning here.\n",
    "\n",
    "(a) With the original train and test sets, run a Random Forest model on the data and report the accuracy score. How does it compare to the scores in Question 3?\n",
    "\n",
    "(b) Create the train and validation sets again, and create a Random Forest Classifier with the following parameters: **n_estimators, max_leaf_nodes, max_depth**, with some values. What do this parameters do? (Again look up the documentation) Run the model and see how the accuracy changes. Change the values and try to get a higher accuracy.\n",
    "\n",
    "(c) Find the best parameters for optimal in accuracy. How does this accuracy compare to the others we have seen?\n",
    "\n",
    "Note: This kind of parameter optimization can be done using built in python functions, GridSearchCV and RandomSearchCV both are methods that take in some kind of range / distribution for the parameters and finds the best one (and uses cross validation which is a bonus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Notes:\n",
    "\n",
    "- Built from Decision Trees\n",
    "- Trees are not flexible when it comes to classifying random samples\n",
    "- You are selecting features and other things to build the boot-strapped dataset randomly\n",
    "- Random Forests create multiple trees wit varying variables, which helps with its accuracy\n",
    "- Then you run the data on every model, and determine yes and no outcomes for your hypothesis question\n",
    "- Bagging: bootstrapping the data + using the data to make a decision. Typically, ~ 1/3 of the data does not end up in the test data. Out of bag dataset is what its called\n",
    "- We can measure the accuracy of our Random Forest by the proportion of Out-of-Bag samples that were correctly classified by the RF. Incorrectly identified -> Out of Bag Error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4(a)\n",
    "\n",
    "(a) With the original train and test sets, run a Random Forest model on the data and report the accuracy score. How does it compare to the scores in Question 3?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for Random Forest Model is 0.86\n",
      "The Confusion Matrix for Random Forest Model is [[58  6]\n",
      " [ 8 28]]\n"
     ]
    }
   ],
   "source": [
    "# 4(a) With the original train and test sets, run a Random Forest model on the data and report the accuracy score. How does it compare to the scores in Question 3?\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "# Building the Random Forest Model\n",
    "model = RandomForestClassifier(n_estimators=100) \n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "# Calculating Accuracy Score and Confusion Matrix\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "print(f'The Accuracy Score for Random Forest Model is {acc_score}')\n",
    "print(f'The Confusion Matrix for Random Forest Model is {conf_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "\n",
    "Using the original train and test sets, the Random Forest model achieved an accuracy of 86% on the test set.\n",
    "\n",
    "Comparison:\n",
    "* Random Forest: 86% accuracy\n",
    "* Tuned Decision Tree: 86.3% accuracy\n",
    "* Untuned Decision Tree: 80% accuracy\n",
    "\n",
    "\n",
    "The Random Forest outperformed the Decision Tree models, highlighting its strength as an ensemble method that reduces variance and typically offers better generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4(b)\n",
    "\n",
    "* Create the train and validation sets again, and create a Random Forest Classifier with the following parameters: **n_estimators, max_leaf_nodes, max_depth**, with some values. What do this parameters do? (Again look up the documentation) Run the model and see how the accuracy changes. Change the values and try to get a higher accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for this Random Forest model is 0.84\n",
      "The Confusion Matrix for this Random Forest model is [[59  5]\n",
      " [11 25]]\n"
     ]
    }
   ],
   "source": [
    "# 4(b) (b) Create the train and validation sets again, and create a Random Forest Classifier with the following parameters: **n_estimators, max_leaf_nodes, max_depth**, with some values. \n",
    "# What do this parameters do? (Again look up the documentation) Run the model and see how the accuracy changes. Change the values and try to get a higher accuracy.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "# Building the Random Forest Model\n",
    "\n",
    "model = RandomForestClassifier(n_estimators= 150, max_leaf_nodes=20, max_depth=10)\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "# Calculating the Accuracy Scores and Confusion Matrix\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "# Printing the outputs\n",
    "print(f'The Accuracy Score for this Random Forest model is {acc_score}')\n",
    "print(f'The Confusion Matrix for this Random Forest model is {conf_matrix}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: \n",
    "With the specified parameters (n_estimators=150, max_leaf_nodes=20, and max_depth=10), the accuracy on the validation set is 84%. This iteration of the model performs worse than the untuned Random Forest\n",
    "\n",
    "Explanation of the Parameters:\n",
    "* n_estimators: This is the number of trees in the Random Forest. More trees generally improve performance but also increase computation time.\n",
    "* max_leaf_nodes: This limits the number of leaf nodes per tree, preventing overly complex trees and reducing the risk of overfitting.\n",
    "* max_depth: This sets the maximum depth of each tree. Limiting the depth helps control the complexity of the model and prevents overfitting on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4(c)\n",
    "\n",
    "* (c) Find the best parameters for optimal in accuracy. How does this accuracy compare to the others we have seen?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "9 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_leaf_nodes' parameter of RandomForestClassifier must be an int in the range [2, inf) or None. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1102: UserWarning: One or more of the test scores are non-finite: [0.81691919 0.82449495 0.81565657 0.79040404 0.79166667 0.81565657\n",
      " 0.80681818 0.81691919 0.79924242        nan        nan        nan\n",
      " 0.8219697  0.81313131 0.81439394 0.81944444 0.7260101  0.81818182\n",
      " 0.82323232 0.81944444]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(max_depth=10,\n",
       "                                                    max_leaf_nodes=20,\n",
       "                                                    n_estimators=150),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                                      13, 14],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [1, 2, 3, 4, 5, 6, 7,\n",
       "                                                           8, 9, 10, 11, 12, 13,\n",
       "                                                           14, 15, 16, 17, 18,\n",
       "                                                           19],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 110, 120, 130,\n",
       "                                                         140]},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomizedSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(max_depth=10,\n",
       "                                                    max_leaf_nodes=20,\n",
       "                                                    n_estimators=150),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={&#x27;max_depth&#x27;: [5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                                      13, 14],\n",
       "                                        &#x27;max_leaf_nodes&#x27;: [1, 2, 3, 4, 5, 6, 7,\n",
       "                                                           8, 9, 10, 11, 12, 13,\n",
       "                                                           14, 15, 16, 17, 18,\n",
       "                                                           19],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 110, 120, 130,\n",
       "                                                         140]},\n",
       "                   scoring=&#x27;accuracy&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: RandomForestClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=8, max_leaf_nodes=13, n_estimators=140)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;RandomForestClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(max_depth=8, max_leaf_nodes=13, n_estimators=140)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestClassifier(max_depth=10,\n",
       "                                                    max_leaf_nodes=20,\n",
       "                                                    n_estimators=150),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={'max_depth': [5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                                      13, 14],\n",
       "                                        'max_leaf_nodes': [1, 2, 3, 4, 5, 6, 7,\n",
       "                                                           8, 9, 10, 11, 12, 13,\n",
       "                                                           14, 15, 16, 17, 18,\n",
       "                                                           19],\n",
       "                                        'n_estimators': [100, 110, 120, 130,\n",
       "                                                         140]},\n",
       "                   scoring='accuracy')"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters to keep in mind: n_estimators=150, max_leaf_nodes=20, and max_depth=10\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "# Building a model to find the best parameters\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': list(range(100,150,10)),\n",
    "    'max_leaf_nodes': list(range(1,20)),\n",
    "    'max_depth': list(range(5,15))\n",
    "}\n",
    "\n",
    "# now we will build the RandomizedSearchCV model\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator = model,\n",
    "    param_distributions = param_dist,\n",
    "    cv = 3,\n",
    "    n_iter = 20,\n",
    "    scoring = 'accuracy')\n",
    "\n",
    "# Fitting the model\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Finding the best parameter for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score for Random Forest model is 0.83\n",
      "The Confusion Matrix for Random Forest model is \n",
      "[[58  6]\n",
      " [11 25]]\n"
     ]
    }
   ],
   "source": [
    "# Building Random Forest model using best parameters. \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "titanic_train = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/train_data.csv')\n",
    "titanic_test = pd.read_csv('/Users/maidaraza/Desktop/Tufts/Course Content/DATA 0201 - Introduction to Python and Machine Learning/Titanic Dataset/test_data.csv')\n",
    "\n",
    "# Dropping the first two columns from the dataset\n",
    "titanic_train = titanic_train.iloc[:, 2:]\n",
    "titanic_test = titanic_test.iloc[:, 2:]\n",
    "\n",
    "# Dividing the dataset into features and target variable\n",
    "x_train = titanic_train.drop('Survived', axis=1)\n",
    "y_train = titanic_train['Survived']\n",
    "\n",
    "x_test = titanic_test.drop('Survived', axis=1)\n",
    "y_test = titanic_test['Survived']\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=140, max_leaf_nodes=13, max_depth=8)\n",
    "model.fit(x_train, y_train)\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "conf_matrix= confusion_matrix(y_test, y_predict)\n",
    "\n",
    "# Printing the outputs\n",
    "print(f'The Accuracy Score for Random Forest model is {acc_score}')\n",
    "print(f'The Confusion Matrix for Random Forest model is \\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: \n",
    "\n",
    "The best parameter combination found for the Random Forest on the validation set is:\n",
    "* n_estimators: 140\n",
    "* max_leaf_nodes: 13\n",
    "* max_depth: 9\n",
    "\n",
    "This achieved a validation accuracy of 83%.\n",
    "\n",
    "Comparison:\n",
    "* Untuned Decision Tree: 80% accuracy\n",
    "* Tuned Decision Tree: 86% accuracy\n",
    "* Untuned Random Forest: 86% accuracy\n",
    "* Tuned Random Forest: 83% accuracy\n",
    "\n",
    "In this case, the untuned Random Forest slightly outperformed the tuned versions of both models. This could be due to the simplicity of the dataset, where extensive tuning might not provide additional benefits and instead harms the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "Come up with an analogy for decision tree's v. random forest's and why random forests avoid the problem of overfitting. (+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of a Decision Tree as a single judge in a courtroom making a ruling based solely on their own perspective. If this judge has a lot of information (i.e., no restrictions like max_depth), they may try to account for every tiny detail, even if those details are just noise or anomalies. This leads to overfitting — making a decision based on specifics that might not apply in general cases.\n",
    "\n",
    "Now imagine a Random Forest as a panel of judges, where each judge gets only a random subset of the evidence (features) and makes their ruling independently. The panel then takes a vote on the final decision. Because each judge sees different parts of the case and they all contribute equally, the final verdict is a blend of their opinions. This process reduces the risk of overfitting because:\n",
    "\n",
    "Diverse Opinions: By giving each judge (tree) different evidence (features), we prevent any single judge from becoming too specific to the details (noise) of a particular case.\n",
    "\n",
    "Averaging Effect: The ensemble decision (average of all trees' predictions) is more robust, reducing the impact of any one judge that might have made a biased or overfitted decision.\n",
    "\n",
    "In essence, a Random Forest is like a team of experts who make more balanced and general decisions than any single expert could on their own. This leads to better generalization on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
